{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNTuJHXCtFfzTCT8/hsLV0Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ovuiproduction/EDI_1701/blob/main/SIH1701_Model_v_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L5Wwd1hp7cpe",
        "outputId": "35bb91b0-2847-4bc3-ed44-e08377f6aafe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m629.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m396.4/396.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers langchain pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pbU7Wv3YAKhy",
        "outputId": "5fcda2f7-1130-4e72-b4b5-72c95b6f59a7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.16)\n",
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.4-py3-none-any.whl.metadata (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: numpy<2.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.32)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.38 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.38)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.4)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.110)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Collecting pdfminer.six==20231228 (from pdfplumber)\n",
            "  Downloading pdfminer.six-20231228-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from pdfplumber) (9.4.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.3.2)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.38->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.38->langchain) (3.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading pdfplumber-0.11.4-py3-none-any.whl (59 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer.six-20231228-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20231228 pdfplumber-0.11.4 pypdfium2-4.30.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "import pdfplumber\n",
        "import numpy as np\n",
        "import torch"
      ],
      "metadata": {
        "id": "hix2wjH7AhUl"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def pdf_loader(file_path):\n",
        "#     text = \"\"\n",
        "#     with pdfplumber.open(file_path) as pdf:\n",
        "#         for page in pdf.pages:\n",
        "#             text += page.extract_text()\n",
        "#     return [Document(page_content=text)]\n",
        "\n",
        "# # Load the PDF\n",
        "# document = pdf_loader(\"/content/shankari_prasad.pdf\")"
      ],
      "metadata": {
        "id": "3mkJF5fsAcHm"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the text into smaller chunks\n",
        "# text_splitter = RecursiveCharacterTextSplitter(\n",
        "#     chunk_size=500,\n",
        "#     chunk_overlap=0,\n",
        "#     length_function=len,\n",
        "# )\n",
        "# docs = text_splitter.split_documents(document)"
      ],
      "metadata": {
        "id": "xnU7mfDWAp_Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_loader(file_path):\n",
        "    text = \"\"\n",
        "    with pdfplumber.open(file_path) as pdf:\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    return text"
      ],
      "metadata": {
        "id": "YRjHb7Lmmkii"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF\n",
        "document = pdf_loader(\"/content/shankari_prasad.pdf\")"
      ],
      "metadata": {
        "id": "g7C3xd1hf0OT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from langchain.text_splitter import TextSplitter\n",
        "from langchain.docstore.document import Document\n",
        "\n",
        "# Define the section headers to split on\n",
        "SECTION_HEADERS = [\n",
        "    \"PETITIONER:\",\n",
        "    \"RESPONDENT:\",\n",
        "    \"DATE OF JUDGMENT:\",\n",
        "    \"BENCH:\",\n",
        "    \"CITATION:\",\n",
        "    \"ACT:\",\n",
        "    \"HEADNOTE:\",\n",
        "    \"JUDGMENT:\"\n",
        "]\n",
        "\n",
        "# Function to split text based on section headers\n",
        "def split_text_by_sections(text):\n",
        "    # Create a regular expression pattern to match section headers\n",
        "    pattern = '|'.join(re.escape(header) for header in SECTION_HEADERS)\n",
        "    sections = re.split(pattern, text, flags=re.IGNORECASE)\n",
        "    # Return non-empty sections only\n",
        "    return [section.strip() for section in sections if section.strip()]"
      ],
      "metadata": {
        "id": "FESRdmk3eyUO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sections = split_text_by_sections(document)\n",
        "docs =  [Document(page_content=section) for section in sections]"
      ],
      "metadata": {
        "id": "akMXV9Xwf4uA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "k40k_SDYgeIh",
        "outputId": "1d995b89-7c0a-46f3-a55d-f74c5ebf7cce"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='SRI SANKARI PRASAD SINGH DEO\\nVs.')"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xY_MGXJoXzD",
        "outputId": "31a9653b-eae0-440d-d05d-b46663b2cbce"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='UNION OF INDIA AND STATE OF BIHAR(And Other Cases).')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqvV5oBioXvU",
        "outputId": "67554d3d-cc5e-4640-dcc5-1f1638810103"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='05/10/1951')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[4]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJNgc8U4oXsb",
        "outputId": "ead5eabe-c8de-4de9-adcd-81a582b1e0cd"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='SASTRI, M. PATANJALI')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "geJI09LdoXpV",
        "outputId": "a03dd729-b96b-4619-88dc-0a74b18ed4e0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='SASTRI, M. PATANJALI\\nKANIA, HIRALAL J. (CJ)\\nMUKHERJEA, B.K.\\nDAS, SUDHI RANJAN\\nAIYAR, N. CHANDRASEKHARA')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "te7xS07ZoXba",
        "outputId": "c1a89019-4f1b-4ac0-b851-0448cd855965"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='1951 AIR 458 1952 SCR 89\\nCITATOR INFO :\\nF 1952 SC 252 (1,30)\\nRF 1954 SC 257 (4)\\nR 1959 SC 395 (28)\\nE&D 1959 SC 512 (4)\\nF 1965 SC 845 (20,21,23,24,25,27,33,35,38,39\\nR 1965 SC1636 (25)\\nO 1967 SC1643 (12,14,23,27,43,44,56,59,61,63\\nRF 1973 SC1461 (16,20,27,30,32,38,39,44,46,88\\nRF 1975 SC1193 (17)\\nRF 1975 SC2299 (649)\\nRF 1980 SC1789 (96)\\nRF 1980 SC2056 (61)\\nRF 1980 SC2097 (6)\\nD 1981 SC 271 (19,33,42,43)\\nRF 1986 SC1272 (78)\\nRF 1986 SC1571 (34)\\nRF 1987 SC1140 (3)')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[7]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtayREwaojAL",
        "outputId": "8151b1f5-9b46-483e-f7f0-e588802970ee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='Constitution (First Amendment) Act, 1951, Arts. 31A,\\n31B-Validity--Constitution of India, 1950, Arts. 13(2),\\n368, 379, 392--Provisional Parliament--Power to amend\\nConstitution- Constitution (Removal of Difficulties) Order\\nNo. 2 of 1950--Validity --Amendment of Constitution--Proce-\\ndure--Bill amended by Legislature--Amendment curtailing\\nfundamental rights--Amendment affecting land--Validity of\\nAmending Act.')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGpeCb7koix2",
        "outputId": "1ccafe89-4f07-41bc-fbd1-bc515ea01563"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(page_content='The Constitution (First Amendment) Act, 1951, which has\\ninserted, inter alia, Arts. 31A and 3lB in the Constitution\\nof India is not ultra vires or unconstitutional.\\nThe provisional Parliament is competent to exercise the\\npower of amending the Constitution under Art. 368. The fact\\nthat the said article refers to the two Houses of the Par-\\nliament and the President separately and not to the Parlia-\\nment, does not lead to the inference that the body which is\\ninvested with the power to amend is not the Parliament but a\\ndifferent body consisting of the two Houses.http://JUDIS.NIC.IN SUPREME COURT OF INDIA Page 2 of 14\\nThe words \"all the powers conferred by the provisions of\\nthis Constitution on Parliament\" in Art. 379 are not con-\\nfined to such powers as could be exercised by the provision-\\nal Parliament consisting of a single chamber, but are wide\\nenough to include the power to amend the Constitution con-\\nferred by Art. 368.\\nThe Constitution (Removal of Difficulties) Order No. 2\\nmade by the President on the 26th January, 1950, which\\npurports to adapt Art. 368 by omitting \"either House of\"\\nand \"in each House\" and substituting \"Parliament\" for \"that\\nHouse\" is not\\n12\\n90\\nbeyond the powers conferred on him by Art. 39:1 and ultra\\nvires. There is nothing in Art. 392 to suggest that the\\nPresident should wait, before adapting a particular article,\\ntill the occasion actually arose for the provisional Parlia-\\nment to exercise the power conferred by the article.\\nThe view that Art. 368 is a complete code in itself in\\nrespect of the procedure provided by it and does not contem-\\nplate any amendment of a Bill for amendment of the Constitu-\\ntion after it has been introduced, and that if the Bill is\\namended during its passage through the House, the amendment\\nAct cannot be said to have been passed in conformity with\\nthe procedure prescribed by Art. 368 and would be invalid,\\nis erroneous.\\nAlthough \"law\" must ordinarily include constitutional\\nlaw there is a clear demarcation between ordinary law which\\nis made in the exercise of legislative power and constitu-\\ntional law, which is made in the exercise of constituent\\npower. In the context of Art. 13, \"law\" must be taken to\\nmean rules or regulations made in exercise of ordinary\\nlegislative power and not amendments to the constitution\\nmade in the exercise of constituent power with the result\\nthat Art. 13(2) does not affect amendments made under Art.\\n368.\\nArticles 31A and 3lB inserted in the Constitution by the\\nConstitution (First Amendment) Act, 1951, do not curtail\\nthe powers of the High Court under Art. 226 to issue writs\\nfor enforcement of any of the rights conferred by Part III\\nor of the Supreme Court under Arts. 132 and 136 to entertain\\nappeals from orders issuing or refusing such writs; but they\\nonly exclude from the purview of Part III ’certain classes\\nof cases. These articles therefore do not require ratifica-\\ntion under cl. (b) of the proviso to Art. 368.\\nArticles 31A and 31B are not invalid on the ground that\\nthey relate to land which is a matter covered by the State\\nList (item 18 of List II) as these articles are essentially\\namendments of the Constitution, and Parliament alone has\\nthe power to enact them.')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sxuu7qUzissZ",
        "outputId": "1e4fba2f-6f04-4ba5-f89f-593d78e965e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load LegalBERT model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "model = BertModel.from_pretrained('nlpaueb/legal-bert-base-uncased')"
      ],
      "metadata": {
        "id": "8V2po3utB9aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(texts):\n",
        "    inputs = tokenizer(texts, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    # Use the mean of the last hidden states as embeddings\n",
        "    embeddings = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "    return embeddings"
      ],
      "metadata": {
        "id": "UwQEatd0B9XL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate embeddings for each document\n",
        "texts = [doc.page_content for doc in docs]\n",
        "embeddings = get_embeddings(texts)"
      ],
      "metadata": {
        "id": "8ZnvhsiEB9Tu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize FAISS index\n",
        "embedding_dim = embeddings.shape[1]\n",
        "faiss_index = faiss.IndexFlatL2(embedding_dim)\n",
        "faiss_index.add(np.array(embeddings, dtype=np.float32))"
      ],
      "metadata": {
        "id": "_GtgeCs3B9Kd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create FAISS vector store\n",
        "class FAISSVectorStore(FAISS):\n",
        "    def __init__(self, index, documents):\n",
        "        self.index = index\n",
        "        self.docstore = {i: doc for i, doc in enumerate(documents)}\n",
        "        self.index_to_docstore_id = {i: i for i in range(len(documents))}"
      ],
      "metadata": {
        "id": "2RreE4vlCRbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the FAISS vector store\n",
        "vector_store = FAISSVectorStore(faiss_index, [doc.page_content for doc in docs])"
      ],
      "metadata": {
        "id": "YXJgBrWDCT42"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import torch\n",
        "import faiss\n",
        "\n",
        "# Function to get embeddings for a single query\n",
        "def get_query_embedding(query, tokenizer, model):\n",
        "    inputs = tokenizer(query, return_tensors='pt', padding=True, truncation=True)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    embedding = outputs.last_hidden_state.mean(dim=1).numpy()\n",
        "    return embedding\n"
      ],
      "metadata": {
        "id": "NTDusrIbDlW9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_ans(query) :\n",
        "  # Generate the query embedding\n",
        "  query_embedding = get_query_embedding(query, tokenizer, model)\n",
        "\n",
        "  # Perform similarity search with FAISS\n",
        "  k = 5  # Number of nearest neighbors to retrieve\n",
        "  distances, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k)\n",
        "\n",
        "  # Retrieve the most similar documents\n",
        "  most_similar_docs = [docs[i] for i in indices[0]]\n",
        "\n",
        "  # Print out the most similar documents\n",
        "  for i, doc in enumerate(most_similar_docs):\n",
        "      print(f\"Document {i}:\")\n",
        "      print(doc.page_content)\n",
        "      print()"
      ],
      "metadata": {
        "id": "m-OAVM8MMzix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the query\n",
        "query = \"Extract the HEADNOTE from the following Supreme Court judgment\"\n",
        "get_ans(query)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "0_74YK-SE4Kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the query\n",
        "query = \"Extract all ACT, laws, IPC sections , constitutional articles/acts, legal frameworks and legal statute mentioned in the text\"\n",
        "get_ans(query)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "bEy3uM36NU-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the query\n",
        "query = \"extract the BENCH from text\"\n",
        "get_ans(query)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "9otkYvFSOB_o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the query\n",
        "query = \"extract Judgement from the text\"\n",
        "get_ans(query)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "w25BDl0cOLGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xRmtf0AUD5UP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_pdfs(pdf_path1, pdf_path2):\n",
        "    # Load and process the first PDF\n",
        "    document1 = pdf_loader(pdf_path1)\n",
        "    sections1 = split_text_by_sections(document1)\n",
        "    docs1 =  [Document(page_content=section) for section in sections1]\n",
        "    text1 = [doc.page_content for doc in docs1]\n",
        "    embeddings1 = get_embeddings(text1)\n",
        "\n",
        "    # Load and process the second PDF\n",
        "    document2 = pdf_loader(pdf_path2)\n",
        "    sections2 = split_text_by_sections(document2)\n",
        "    docs2 =  [Document(page_content=section) for section in sections2]\n",
        "    text2 = [doc.page_content for doc in docs2]\n",
        "    embeddings2 = get_embeddings(text2)\n",
        "\n",
        "    # Initialize FAISS index for both PDFs\n",
        "    embedding_dim = embeddings1.shape[1]\n",
        "    faiss_index2 = faiss.IndexFlatL2(embedding_dim)\n",
        "    faiss_index2.add(np.array(embeddings2, dtype=np.float32))\n",
        "\n",
        "     # # Compare the embeddings\n",
        "    similarity_threshold = 0.5  # Adjust this threshold as needed\n",
        "    total_comparisons = 0\n",
        "    similar_comparisons = 0\n",
        "\n",
        "    # for embedding in embeddings1:\n",
        "    #     distances, indices = faiss_index2.search(np.array([embedding], dtype=np.float32), k=5) # Adjust k as needed\n",
        "    #     for distance in distances[0]:\n",
        "    #         total_comparisons += 1\n",
        "    #         if distance < similarity_threshold:\n",
        "    #             similar_comparisons += 1\n",
        "    for i, embedding in enumerate(embeddings1):\n",
        "        distances, _ = faiss_index2.search(np.array([embedding], dtype=np.float32), k=1)\n",
        "        total_comparisons += 1\n",
        "        print(distances[0][0])\n",
        "        if distances[0][0] < similarity_threshold:\n",
        "            similar_comparisons += 1\n",
        "\n",
        "    # Calculate overall similarity percentage\n",
        "    if total_comparisons > 0:\n",
        "        similarity_percentage = (similar_comparisons / total_comparisons) * 100\n",
        "    else:\n",
        "        similarity_percentage = 0.0\n",
        "\n",
        "    return similarity_percentage"
      ],
      "metadata": {
        "id": "vDvGQ8QTSyeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage\n",
        "similarity_percentage = compare_pdfs(\"/content/sajjan_singh.pdf\",\"/content/shankari_prasad.pdf\")\n",
        "print(f\"Overall Similarity: {similarity_percentage:.2f}%\")"
      ],
      "metadata": {
        "id": "SMcCnFFuUDYg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}